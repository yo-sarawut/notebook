{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yosarawut/notebook/blob/main/model_multi_class_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32E9mS--g4Cu"
   },
   "source": [
    "\n",
    "Multi Class Text Classification\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7hH1DMtg7ht",
    "outputId": "f820c986-b5f6-4400-9c9d-79403cc5c5d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from time import time\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9PNdSjbh1Et"
   },
   "outputs": [],
   "source": [
    "## Load data\n",
    "def load_data_train(files): \n",
    "  print(\"Load DataFrame\")\n",
    "  df = joblib.load(files)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWOMOErppB3g"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "def download_files(urls,paths):\n",
    "  print (\"download start!\")\n",
    "  data = urllib.request.urlretrieve(urls, filename='./content/sample_data')\n",
    " \n",
    "  print (\"download file location: \", paths)\n",
    "  print (\"download complete!\")\n",
    "  \n",
    "\n",
    "  return load_data_train(paths)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vf2QIEmniLiT"
   },
   "source": [
    "## ดึงข้อมูลจาก Excel\n",
    "\n",
    "ดึงข้อมูลจากไฟล์ Excel ที่ upload ไว้บน GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xE0VD-HhILl"
   },
   "outputs": [],
   "source": [
    "def get_datas(urls, sheets, types):\n",
    "    data = \"\"\n",
    "    t0 = time()\n",
    "    a = []\n",
    "    i = 0\n",
    "    for s in sheets:\n",
    "        s = pd.read_excel(urls, sheet_name=s)\n",
    "        s = s[[types, 'description']]\n",
    "        s['description'] = s['description'].str.lower()\n",
    "        if i == 0:\n",
    "            data = s.copy()\n",
    "        else:\n",
    "            data = pd.concat([data, s], ignore_index=True)\n",
    "        i += 1\n",
    "    data.columns = ['target', 'data']\n",
    "    data['target'] = data['target'].apply(str)\n",
    "    load_time = time() - t0\n",
    "    print(\"Load dataset time:  %0.3fs\" % load_time)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_h1Ikt3xhMJM"
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "urls = 'https://github.com/dragon-library/work_space/raw/main/HS_Code/data/hs_code.xlsx'\n",
    "\n",
    "types = 'section'\n",
    "#sheets = ['8_digit','6_digit','4_digit', 'test_01', 'Declaration_2019_10']\n",
    "sheets = ['8_digit','6_digit','4_digit', '2_digit']\n",
    "df = get_datas(urls,sheets,types)\n",
    "\n",
    "print(len(df))\n",
    "df.columns = ['category', 'text']\n",
    "\n",
    "\n",
    "\n",
    "load_time = time() - t0\n",
    "print(\"Load data time:  %0.3fs\" % load_time)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N87GwQBVl-2l"
   },
   "outputs": [],
   "source": [
    "def get_decl(urls, sheets):\n",
    "    data = \"\"\n",
    "    t0 = time()\n",
    "    a = []\n",
    "    i = 0\n",
    "    for s in sheets:\n",
    "        dfs = pd.read_excel(urls, sheet_name=s)\n",
    "        dfs = dfs[['target', 'data']]\n",
    "        dfs['data'] = dfs['data'].str.lower()\n",
    "        if i == 0:\n",
    "            data = dfs.copy()\n",
    "        else:\n",
    "            data = pd.concat([data, dfs], ignore_index=True)\n",
    "        i += 1\n",
    "   \n",
    "    data['target'] = data['target'].apply(str)\n",
    "    load_time = time() - t0\n",
    "    print(\"Load dataset time:  %0.3fs\" % load_time)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpS1mpqqhQ_0"
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "urls = 'https://github.com/dragon-library/work_space/raw/main/HS_Code/data/decl_data.xlsx'\n",
    "sheets = ['decx','deci']\n",
    "df_decl = get_decl(urls,sheets)\n",
    "\n",
    "print(len(df_decl))\n",
    "df_decl.columns = ['category', 'text']\n",
    "load_time = time() - t0\n",
    "print(\"Load data time:  %0.3fs\" % load_time)\n",
    "\n",
    "df_decl.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHwRYnnoh7Cj"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df, df_decl], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLI8Tq94iSBY"
   },
   "source": [
    "## Load Data\n",
    "\n",
    "สามารถนำไฟล์ที่ได้จัดการ clean data พร้อมทั้งจัดรูปแบบตามที่เรากำหนด แล้ว Save ไว้ใช้งานในภายหลัง โดยสามารถ download ข้อมูลดังกล่าวมาใช้ในภายหลัง โดย pack ข้อมูลให้อยู่ในไฟล์ full_train_section.pkl เพียงไฟล์เดียว มีข้อมูลประมาณ 90,000 row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "id": "Xl1usTOr0SQV",
    "outputId": "cd0f83da-d74e-4f44-df58-99f20723c8de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load DataFrame\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56404</th>\n",
       "      <td>16</td>\n",
       "      <td>heating pad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97317</th>\n",
       "      <td>17</td>\n",
       "      <td>gn1z74291a09 a moulding lh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79068</th>\n",
       "      <td>16</td>\n",
       "      <td>machinery for filling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52005</th>\n",
       "      <td>04</td>\n",
       "      <td>peanut butter flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14222</th>\n",
       "      <td>15</td>\n",
       "      <td>steel, alloy; bars and rods, cold-formed or co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88253</th>\n",
       "      <td>16</td>\n",
       "      <td>unit assy, headlamp rh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86561</th>\n",
       "      <td>06</td>\n",
       "      <td>fresh  fish ( threadfin bream.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102280</th>\n",
       "      <td>17</td>\n",
       "      <td>brkt asy rr bmpr cvr mtng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48974</th>\n",
       "      <td>15</td>\n",
       "      <td>aluminium profiles yajk079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85693</th>\n",
       "      <td>06</td>\n",
       "      <td>goat milk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27875</th>\n",
       "      <td>17</td>\n",
       "      <td>pts. &amp; access. of motor vehicles of 8701, neso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52091</th>\n",
       "      <td>07</td>\n",
       "      <td>printing plate 200l30%45 luc3955a-top-sto bran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81341</th>\n",
       "      <td>17</td>\n",
       "      <td>front seat,red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735</th>\n",
       "      <td>08</td>\n",
       "      <td>tanned or crust hides and skins; without hair ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41268</th>\n",
       "      <td>16</td>\n",
       "      <td>assy rodconnecting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82809</th>\n",
       "      <td>07</td>\n",
       "      <td>hose,drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20470</th>\n",
       "      <td>06</td>\n",
       "      <td>diethyl phosphite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52195</th>\n",
       "      <td>15</td>\n",
       "      <td>fitting 45 half coupling 3/4\" 3000lb sw a182-f304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78867</th>\n",
       "      <td>18</td>\n",
       "      <td>semi finished lens sas167      75 ( sas167 ) t2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49932</th>\n",
       "      <td>11</td>\n",
       "      <td>man cut &amp; sew/top/top 100% cotton620969tiv509605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       target                                               data\n",
       "56404      16                                        heating pad\n",
       "97317      17                         gn1z74291a09 a moulding lh\n",
       "79068      16                              machinery for filling\n",
       "52005      04                               peanut butter flavor\n",
       "14222      15  steel, alloy; bars and rods, cold-formed or co...\n",
       "88253      16                             unit assy, headlamp rh\n",
       "86561      06                    fresh  fish ( threadfin bream.)\n",
       "102280     17                          brkt asy rr bmpr cvr mtng\n",
       "48974      15                         aluminium profiles yajk079\n",
       "85693      06                                          goat milk\n",
       "27875      17  pts. & access. of motor vehicles of 8701, neso...\n",
       "52091      07  printing plate 200l30%45 luc3955a-top-sto bran...\n",
       "81341      17                                     front seat,red\n",
       "3735       08  tanned or crust hides and skins; without hair ...\n",
       "41268      16                                 assy rodconnecting\n",
       "82809      07                                         hose,drain\n",
       "20470      06                                  diethyl phosphite\n",
       "52195      15  fitting 45 half coupling 3/4\" 3000lb sw a182-f304\n",
       "78867      18    semi finished lens sas167      75 ( sas167 ) t2\n",
       "49932      11   man cut & sew/top/top 100% cotton620969tiv509605"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files=\"/content/sample_data/full_train_section.pkl\"\n",
    "df = load_data_train(files)\n",
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gce1gYKXo5ht",
    "outputId": "74828214-faa2-4b84-c251-bdcd0ec991e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72185\n",
      "72185\n",
      "72185\n",
      "18047\n",
      "18047\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "embedding_dim = 64\n",
    "max_length = 100\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "training_portion = .8\n",
    "\n",
    "labels = df.target\n",
    "articles = df.data\n",
    "\n",
    "train_size = int(len(articles) * training_portion)\n",
    "\n",
    "train_articles = articles[0: train_size]\n",
    "train_labels = labels[0: train_size]\n",
    "\n",
    "validation_articles = articles[train_size:]\n",
    "validation_labels = labels[train_size:]\n",
    "\n",
    "print(train_size)\n",
    "print(len(train_articles))\n",
    "print(len(train_labels))\n",
    "print(len(validation_articles))\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzmZevxxjyPT"
   },
   "source": [
    "## Clean Data\n",
    "\n",
    "ทำความสะอาดข้อมูล เพื่อเตรียมพร้อมก่อนประมวลผลข้อมูล ได้ทำการ Preprocessing data โดยตรวจสอบรายละเอียดข้อมูลว่า เป็นข้อความภาษาอังกฤษหรือไม่ ถ้าใช่ก็จะนำข้อมูลไปตรวจสอบความถูกต้องของตัวสะกด กำหนดมาตรฐานสร้างโทเค็นและกำหนดเวกเตอร์ข้อมูลโดยใช้เลเยอร์การ preprocessing.TextVectorization\n",
    "\n",
    "- Standardization หมายถึงการประมวลผลข้อความ โดยทั่วไปจะลบเครื่องหมายวรรคตอนหรือองค์ประกอบ HTML เพื่อลดความซับซ้อนของชุดข้อมูล\n",
    "\n",
    "- Tokenization หมายถึงการแยกสตริงออกเป็นโทเค็น (ตัวอย่างเช่นการแยกประโยคออกเป็นคำแต่ละคำโดยการแบ่งช่องว่าง)\n",
    "\n",
    "- Vectorization หมายถึงการแปลงโทเค็นเป็นตัวเลขเพื่อให้สามารถป้อนเข้าใน neural network (โครงข่ายประสาทเทียม) ได้\n",
    "\n",
    "และกำหนดมาตรฐาน ให้แปลงข้อความเป็นตัวพิมพ์เล็กและลบเครื่องหมายวรรคตอนและเริ่มต้นจะแบ่งช่องว่าง และ vectorization จะจัดทำดัชนีคำศัพท์ เพื่อสร้างโมเดลที่คำนึงถึงลำดับคำ เป็นต้น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oL42Hmfht60u",
    "outputId": "7e84e926-a0de-4738-dc30-d80f32b05050"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'aluminium': 9,\n",
       " 'and': 4,\n",
       " 'for': 7,\n",
       " 'in': 8,\n",
       " 'not': 5,\n",
       " 'of': 3,\n",
       " 'or': 2,\n",
       " 'other': 6,\n",
       " 'than': 10}"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_articles)\n",
    "word_index = tokenizer.word_index\n",
    "dict(list(word_index.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shqVrZNFt63M",
    "outputId": "37491927-ab61-47d5-c2d4-6f4d7ea1b4d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[568, 640, 303, 6, 10, 4705, 4, 1190, 2804, 1733, 640]\n"
     ]
    }
   ],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_articles)\n",
    "print(train_sequences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FI0LUoI9t653",
    "outputId": "7c0a2fc9-48e2-4107-c78c-02c85b171fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "100\n",
      "8\n",
      "100\n",
      "11\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(len(train_sequences[0]))\n",
    "print(len(train_padded[0]))\n",
    "\n",
    "print(len(train_sequences[1]))\n",
    "print(len(train_padded[1]))\n",
    "\n",
    "print(len(train_sequences[10]))\n",
    "print(len(train_padded[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qVyox_dsXB7"
   },
   "source": [
    "ตัวอย่างข้อมูล ที่ทำการ Preprocessing data แล้ว ก่อนจะนำข้อมูลเข้าไป train model ต่อไป"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4E4ihnbJuUZU",
    "outputId": "31420c48-70bb-4f4a-9631-4838ea742430"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 568  640  303    6   10 4705    4 1190 2804 1733  640    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_padded[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIW_rq7fuUb1",
    "outputId": "37026e61-881b-46aa-9dae-206382c1cab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18047\n",
      "(18047, 100)\n"
     ]
    }
   ],
   "source": [
    "#train_sequences = tokenizer.texts_to_sequences(train_articles)\n",
    "#train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation_articles)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(len(validation_sequences))\n",
    "print(validation_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SqN7_7puUfN",
    "outputId": "09ed2d1c-029b-46d5-b6c0-83f36d5cbed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\n",
      "[11]\n",
      "[11]\n",
      "(72185, 1)\n",
      "[1]\n",
      "[1]\n",
      "[2]\n",
      "(18047, 1)\n"
     ]
    }
   ],
   "source": [
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
    "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\n",
    "print(training_label_seq[0])\n",
    "print(training_label_seq[1])\n",
    "print(training_label_seq[2])\n",
    "print(training_label_seq.shape)\n",
    "\n",
    "print(validation_label_seq[0])\n",
    "print(validation_label_seq[1])\n",
    "print(validation_label_seq[2])\n",
    "print(validation_label_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cy9jdlkLuUjX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ef0Q_-YtS3X",
    "outputId": "8c18de97-5209-4c01-a660-4d8fcb8bf9b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bovine animals live other than cattle and buffalo purebred breeding animals ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "---\n",
      "bovine animals; live, other than cattle and buffalo - purebred breeding animals\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_article(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "print(decode_article(train_padded[10]))\n",
    "print('---')\n",
    "print(train_articles[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkByGLcjZ80u"
   },
   "outputs": [],
   "source": [
    "def load_model(paths=None):\n",
    "  loaded_keras_model = tf.keras.models.load_model(paths)\n",
    "\n",
    "  return loaded_keras_model\n",
    "\n",
    "\n",
    "def save_model(models,paths = None):\n",
    "  keras_model_path = paths\n",
    "  models.save(keras_model_path)\n",
    "  print(\"save model success\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mw9wzeI8uUmP",
    "outputId": "c0ef7dfd-8bcb-4794-d39f-dc0d4ad3bc1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          320000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25)                1625      \n",
      "=================================================================\n",
      "Total params: 395,929\n",
      "Trainable params: 395,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    # use ReLU in place of tanh function since they are very good alternatives of each other.\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    # Add a Dense layer with 6 units and softmax activation.\n",
    "    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n",
    "    tf.keras.layers.Dense(25, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQVgmFNpsq01"
   },
   "source": [
    "## Train Model\n",
    "\n",
    "โดยครั้งนี้ได้กำหนดให้ train 20 รอบ โดยแต่ละรอบระบบจะปรับปรุง parametor ต่าง ๆ เพื่อให้ model มีความแม่นยำขึ้น สังเกตุได้จากค่า accuracy ที่เพิ่มขึ้นเรื่อย ๆ และค่า loss ก็จะลดลงเรื่อย ๆ ตามจำนวนรอบในการ Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aAMtHDVhursa",
    "outputId": "1070b9ff-ed00-4f3b-c0ea-27133ae01cda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2256/2256 - 215s - loss: 1.0134 - accuracy: 0.7007 - val_loss: 1.4114 - val_accuracy: 0.5726\n",
      "Epoch 2/20\n",
      "2256/2256 - 220s - loss: 0.5562 - accuracy: 0.8322 - val_loss: 1.4005 - val_accuracy: 0.5774\n",
      "Epoch 3/20\n",
      "2256/2256 - 215s - loss: 0.4796 - accuracy: 0.8508 - val_loss: 1.4071 - val_accuracy: 0.5825\n",
      "Epoch 4/20\n",
      "2256/2256 - 214s - loss: 0.4314 - accuracy: 0.8628 - val_loss: 1.4019 - val_accuracy: 0.5880\n",
      "Epoch 5/20\n",
      "2256/2256 - 215s - loss: 0.3936 - accuracy: 0.8729 - val_loss: 1.4775 - val_accuracy: 0.5910\n",
      "Epoch 6/20\n",
      "2256/2256 - 215s - loss: 0.3627 - accuracy: 0.8823 - val_loss: 1.5500 - val_accuracy: 0.5849\n",
      "Epoch 7/20\n",
      "2256/2256 - 215s - loss: 0.3362 - accuracy: 0.8902 - val_loss: 1.5878 - val_accuracy: 0.5874\n",
      "Epoch 8/20\n",
      "2256/2256 - 215s - loss: 0.3126 - accuracy: 0.8969 - val_loss: 1.7501 - val_accuracy: 0.5824\n",
      "Epoch 9/20\n",
      "2256/2256 - 215s - loss: 0.2914 - accuracy: 0.9042 - val_loss: 1.8286 - val_accuracy: 0.5729\n",
      "Epoch 10/20\n",
      "2256/2256 - 215s - loss: 0.2716 - accuracy: 0.9097 - val_loss: 1.9229 - val_accuracy: 0.5773\n",
      "Epoch 11/20\n",
      "2256/2256 - 215s - loss: 0.2544 - accuracy: 0.9150 - val_loss: 2.0071 - val_accuracy: 0.5757\n",
      "Epoch 12/20\n",
      "2256/2256 - 216s - loss: 0.2383 - accuracy: 0.9207 - val_loss: 2.1888 - val_accuracy: 0.5678\n",
      "Epoch 13/20\n",
      "2256/2256 - 220s - loss: 0.2255 - accuracy: 0.9244 - val_loss: 2.2432 - val_accuracy: 0.5654\n",
      "Epoch 14/20\n",
      "2256/2256 - 216s - loss: 0.2135 - accuracy: 0.9288 - val_loss: 2.3887 - val_accuracy: 0.5681\n",
      "Epoch 15/20\n",
      "2256/2256 - 216s - loss: 0.1999 - accuracy: 0.9326 - val_loss: 2.4390 - val_accuracy: 0.5713\n",
      "Epoch 16/20\n",
      "2256/2256 - 222s - loss: 0.1907 - accuracy: 0.9358 - val_loss: 2.5826 - val_accuracy: 0.5636\n",
      "Epoch 17/20\n",
      "2256/2256 - 215s - loss: 0.1819 - accuracy: 0.9383 - val_loss: 2.6844 - val_accuracy: 0.5635\n",
      "Epoch 18/20\n",
      "2256/2256 - 215s - loss: 0.1756 - accuracy: 0.9407 - val_loss: 2.7466 - val_accuracy: 0.5663\n",
      "Epoch 19/20\n",
      "2256/2256 - 220s - loss: 0.1675 - accuracy: 0.9433 - val_loss: 2.9017 - val_accuracy: 0.5591\n",
      "Epoch 20/20\n",
      "2256/2256 - 216s - loss: 0.1616 - accuracy: 0.9450 - val_loss: 2.9471 - val_accuracy: 0.5649\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "num_epochs = 20\n",
    "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEVaW19Zthlv"
   },
   "source": [
    "เมื่อ train เรียบร้อย ผลลัพธ์คือได้ model ที่มีค่า accuracy 94.50% ใช้เวลาในการ train รอบละประมาณ 3 นาทีกว่า ๆ รวมก็ประมาณ 1 ชั่วโมงนิด ๆ เมื่อ Train เสร็จเราสามารถ save model ดังกล่าวเพื่อนำไปใช้ในการทำนาย หรือเก็บไว้เพื่อใช้ในการเรียนเพิ่มเติมในอนาคตได้  โดยจัดเก็บเป็นไฟล์ model_master.pkl เพียงไฟล์เดียว มีขนาดไม่เกิน 20 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olecvibAuvhi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGt0ME0cgn_j"
   },
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vpEroK_ygrEI",
    "outputId": "bc8903b1-3c61-4539-e6d6-05c0c6e7af26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /content/data/assets\n",
      "save model success\n"
     ]
    }
   ],
   "source": [
    " #save the model\n",
    " print(\"Save Model\")\n",
    "\n",
    " joblib.dump(model, 'data/model/model_master.pkl', compress=1)\n",
    " print(\"Create Model.. success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phNqyEOpzI8h"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "multi_class_text_classification.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
